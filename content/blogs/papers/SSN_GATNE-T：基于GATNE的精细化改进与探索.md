---
title: "SSN_GATNE-T：基于GATNE的精细化改进与探索"
showAuthor: false
date: 2022-07-25
description: "SSN_GATNE-T"
slug: "SSN_GATNE-T-VS-GATNE-T"
tags: ["论文", "KG", "表示学习"]
# series: [""]
# series_order: 1
# weight: 4
draft: false
---

<!-- 渲染公式 -->
{{< katex >}}

> [Recommendation algorithm based on attributed multiplex heterogeneous network](https://peerj.com/articles/cs-822/)


在处理真实世界中复杂的 **属性化多重异构网络（AMHEN）** 时，清华大学和阿里巴巴提出了GATNE模型。它通过巧妙的基向量与边向量分解，并引入自注意力机制来动态融合多重关系，为该领域树立了一个强大的基准。

论文《Recommendation algorithm based on attributed multiplex heterogeneous network》提出的 **SSN_GATNE-T** 模型，正是在GATNE框架基础之上的一次精细化探索与改进。该模型并没有颠覆GATNE的核心架构，而是聚焦于其最关键的组件—— **自注意力机制**，通过替换其中的激活函数与归一化函数，旨在更精准地捕获节点间的交互信息，从而提升推荐系统的性能。

---

## 1. 前情回顾：GATNE的核心机制

在深入了解SSN_GATNE-T之前，有必要先回顾GATNE的核心设计。GATNE将一个节点 \(v_i\) 在特定边类型 \(r\) 下的表示 \(v_{i,r}\) 分解为两部分：

1. **基向量 \(b_i\)**: 所有关系类型共享，捕获节点通用特征。
2. **边向量 \(u_{i,p}\)**: 每种关系类型 \(p\) 独有一个，捕获节点在特定关系下的语义。

其最关键的创新在于，当计算节点 \(v_i\) 在目标关系 \(r\) 下的表示时，它使用自注意力机制来动态计算该节点在**所有**关系类型下的边向量 \(\{u_{i,1}, ..., u_{i,m}\}\) 的重要性权重。GATNE原始的注意力系数计算公式如下：

$$
\mathbf{a}_{i,r} = \text{softmax}(\mathbf{w}_r^T \tanh(\mathbf{W}_r \mathbf{U}_i))^T
$$

其中，\(\mathbf{U}i = [u{i,1}, ..., u_{i,m}]$ 是节点所有边向量的集合。这个公式可以分解为两步：

1. **特征变换与激活**：使用`tanh`函数对经过线性变换的边向量特征进行非线性激活。
2. **权重归一化**：使用`softmax`函数将得到的注意力分数转换为一个概率分布，确保所有权重之和为1。

GATNE的这一设计取得了巨大成功，但SSN_GATNE-T的作者认为，这里的激活函数和归一化函数仍有改进空间。

---

### 2. SSN_GATNE-T的核心改进：注意力机制的再思考

SSN_GATNE-T继承了GATNE的整体框架，但对其自注意力机制的计算公式进行了两处关键的修改，旨在“更好地减少潜在用户信息的损失”。其新的注意力系数计算公式为：

$$
\mathbf{a}_{i,r} = \text{softsign}(\mathbf{w}_r^T \text{sigmoid}(\mathbf{W}_r \mathbf{U}_i))^T
$$

对比GATNE的原始公式，变化显而易见：

1. **`tanh` 被 `sigmoid` 替代**
2. **`softmax` 被 `softsign` 替代**

下面详细解读这两处修改的意图和作用。

### 2.1 激活函数：从 `tanh` 到 `sigmoid`

- **GATNE (`tanh`)**: `tanh`函数的输出范围是 `(-1, 1)`，它是一个零点对称的函数。它对特征进行了非线性变换，使其分布在0的周围。
- **SSN_GATNE-T (`sigmoid`)**: `sigmoid`函数的输出范围是 `(0, 1)`。这个范围使得其输出可以被直观地理解为一种“门控”信号或“概率”值。将变换后的边向量特征压缩到 `(0, 1)` 区间，可以看作是在计算每种关系类型的“激活强度”或“相关性得分”。相较于`tanh`，`sigmoid`的输出非负，在某些场景下可能更具解释性。

### 2.2 归一化函数：从 `softmax` 到 `softsign`

这是SSN_GATNE-T最核心的改进。

- **GATNE (`softmax`)**: `softmax`通过指数函数将一组任意实数转换为一个概率分布。它的优点是输出的权重和严格为1，但缺点是具有“赢家通吃”（winner-takes-all）的倾向。由于指数函数的放大效应，一个稍大的输入值会得到一个远大于其他值的概率，这可能导致模型过分关注某一种关系，而忽略其他次要但同样有用的关系信息。
- **SSN_GATNE-T (`softsign`)**: `softsign`是一个非指数型的归一化函数，其数学形式为 \(f(x) = \frac{x}{1+|x|}\)，输出范围是 `(-1, 1)`。与`softmax`相比，`softsign`有几个关键特性：
    - **平滑性**：它比`softmax`更加平滑，不会因为输入的微小差异而产生剧烈的输出变化，对异常值不那么敏感。
    - **无“赢家通吃”**：它不会强制将最大的权重推向1，而是根据输入值的大小进行平滑缩放。这使得模型可以同时考虑多种不同强度的关系，保留了更丰富的交互信息。论文作者认为，这有助于“减少潜在用户信息的损失”。
    - **计算效率**：不涉及指数运算，计算上可能更高效。

通过这一替换，SSN_GATNE-T的注意力机制在融合多重关系时，策略变得更加“柔和”，能够更好地平衡主要关系和次要关系的影响，从而捕获更全面的用户兴趣。

---

### 3. 模型设计对比：GATNE vs. SSN_GATNE-T

为了更清晰地展示二者的区别，可以总结如下表：

| 设计模块 | GATNE (2019) | SSN_GATNE-T (2021) |
| --- | --- | --- |
| **基本架构** | 基向量 + 边向量分解 | 继承GATNE，保持不变 |
| **边向量聚合** | 基于邻居聚合（如均值） | 继承GATNE，保持不变 |
| **注意力输入激活** | `tanh` | `sigmoid` |
| **注意力权重归一化** | `softmax` | `softsign` |
| **优化器** | Adam | Adam (论文中强调其对快速收敛和调优的帮助) |

从对比中可以看出，SSN_GATNE-T并非对GATNE的颠覆，而是一次精准的“外科手术式”升级。它保留了GATNE强大的表示分解和邻域聚合框架，仅在注意力计算这一核心环节进行了函数级的优化。

---

### 4. 实验与结论

该论文在Amazon和YouTube这两个公开数据集上进行了实验，并将SSN_GATNE-T与GATNE及其他主流模型进行了对比。实验结果显示，SSN_GATNE-T在ROC-AUC、PR-AUC和F1-score等所有评估指标上均取得了优于GATNE的性能。

特别是在F1-score指标上，提升尤为明显。这表明通过`softsign`函数平衡不同关系类型的权重，确实有助于模型做出更准确的预测。消融实验也证实，`sigmoid`和`softsign`的引入都对模型性能有正向贡献。

**结论可以总结为：**

1. **精细化改进的有效性**：SSN_GATNE-T证明了，即使在GATNE这样强大的基线上，通过对核心组件（如注意力机制中的激活与归一化函数）进行有针对性的、细微的调整，依然可以获得可观的性能提升。
2. **`softsign`的潜力**：该研究揭示了`softsign`作为`softmax`替代方案在图注意力网络中的潜力。它提供了一种更平滑、鲁棒性更好的方式来融合多源信息，尤其适用于存在多种强度不一关系的多重网络。
3. **对复杂推荐场景的价值**：通过更有效地挖掘和融合用户与物品之间的多重交互信息，SSN_GATNE-T为解决大规模、复杂网络环境下的推荐问题，特别是冷启动问题，提供了新的思路。
