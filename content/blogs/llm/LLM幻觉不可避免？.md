---
title: "LLM幻觉不可避免？"
showAuthor: false
date: 2025-08-16
description: "LLM幻觉不可避免"
slug: "llm-hallucinations-taxonomy"
tags: ["论文", "LLM", "幻觉"]
# series: [""]
# series_order: 1
draft: false
---

<!-- 渲染公式 -->
{{< katex >}}


<!-- # LLM幻觉不可避免？ -->


> **Arxiv**: [A comprehensive taxonomy of hallucinations in Large Language Models
](https://arxiv.org/abs/2508.01781v1)

大型语言模型（LLM）彻底改变了自然语言处理领域，但其产生“幻觉”——即生成看似合理但事实错误或完全捏造的内容——的倾向，仍然是一个严峻的挑战。这种现象不仅是技术上的小瑕疵，更是关乎模型可靠性、安全性和信任度的核心问题。本文基于最新的学术研究，旨在全面、系统地剖析LLM幻觉的定义、分类、成因、评估方法及缓解策略。


### 一、定义LLM幻觉及其必然性

**1. 概念性定义**

在LLM领域，幻觉被广泛理解为模型生成了“看似合理但并非事实的内容”。这意味着模型产生的输出可能是“错误的、不相关的，或者根本不符合事实逻辑的”。这与医学上指没有外部刺激的感官体验不同，LLM的幻觉是指在回应用户查询时，创造了非事实信息，并且通常不明确说明其内容的虚构性。

**2. 形式化定义与理论必然性**

论文《幻觉是不可避免的：大型语言模型的一种内在局限性》提供了一个严谨的形式化框架，从可计算性理论的角度论证了幻觉的必然性。

**形式化定义：**

幻觉被形式化地定义为一个可计算的LLM（表示为 **h**）与一个可计算的基准真相函数（表示为 **f**）之间的不一致性。

- **f 的形式化世界 (基准真相函数)**: 这个世界被概念化为一个集合 \(G_f = \{(s, f(s))|s \in S\}\)，其中对于任何来自所有有限长度字符串集合 **S** 的输入字符串 **s**，**f(s)** 代表唯一的正确输出。
- **幻觉条件**: 一个LLM **h** 被认为相对于基准真相函数 **f** 产生了“幻觉”，如果在其所有训练阶段中，总存在至少一个输入字符串 **s**，使得LLM的输出 **h(s)** 与正确的输出 **f(s)** 不匹配。形式化表达为：\(\forall i \in \mathbb{N}, \exists s \in S\) 使得 \(h_i \neq f(s)\)。

**理论必然性的推论：**

该框架通过一系列定理指出，幻觉是可计算LLM的一种固有特性，无论其架构、学习算法或训练数据如何。其核心论证根植于对角论证法（diagonalization），这是可计算性理论中的一个经典证明技术。

- **定理1：可计算可枚举的LLM集合必将产生幻觉**：对于任何可计算可枚举的LLM集合（这包括了所有当前的多项式时间有界LLM），都存在一个可计算的基准真相函数 **f**，使得该集合中所有LLM的所有状态都会产生幻觉。
- **定理2：LLM将在无穷多个问题上产生幻觉**：该论断进一步扩展，指出对于任何可计算可枚举的LLM集合，都存在一个基准真相函数 **f**，使得该集合中所有LLM的所有状态都会在无穷多个输入上产生幻觉。
- **定理3：任何可计算的LLM都将产生幻觉**：该定理将结论推广至任何单个的可计算LLM。这意味着，即使是未来的LLM，只要它们是可计算的，就必然会存在使其产生幻觉的基准真相。
- **推论1：无法自我消除幻觉**：一个直接的推论是，所有可计算的LLM在本质上都缺乏阻止自己产生幻觉的能力。这意味着仅依赖LLM内部机制（如基于提示的思维链推理）的缓解策略，无法从根本上消除幻觉。

这些理论（详情见 **表1**）揭示了一个根本性的事实：幻觉并非一个可以通过改进训练或架构就能完全根除的“bug”，而是根植于可计算性本质的内在限制。

![image.png](https://cdn.jsdelivr.net/gh/gongzitaiyi/picture@master/uPic/2025/11/3zK7gL.png)

### 二、LLM幻觉的核心分类法

学术界对LLM幻觉提出了几种关键的分类方法，其中最基础和被广泛接受的两个维度是“内在与外在”和“事实性与忠实性”。

**1. 内在幻觉 (Intrinsic Hallucination) vs. 外在幻觉 (Extrinsic Hallucination)**

这个分类基于生成文本与所提供输入上下文的关系。

- **内在幻觉**: 指生成的文本直接与输入或上下文相矛盾。这种错误源于模型在推理过程中无法保持逻辑一致性。例如，一篇待摘要的文章指出某疫苗于2019年获批，而模型生成的摘要却称该疫苗被拒绝。
- **外在幻觉**: 指生成的文本与训练数据不一致，并且“既不能被输入上下文支持，也无法被其证伪”。这通常涉及引入现实中不存在的实体、事实或事件。例如，声称“巴黎虎于1885年被猎杀至灭绝”，这是一个完全捏造的实体和事件。

**2. 事实性幻觉 (Factuality Hallucination) vs. 忠实性幻觉 (Faithfulness Hallucination)**

这个分类关注生成内容的真实性及其对输入的遵循程度。

- **事实性幻觉**: 指模型生成了“事实错误的内容”，直接与“真实世界知识”或“已建立的验证来源”相矛盾。它关乎内容的绝对正确性。例如，模型声称“查尔斯·林德伯格是第一个登上月球的人”。
- **忠实性幻觉**: 指模型的输出“偏离了输入提示或提供的上下文”。生成的回答可能内部逻辑自洽且看似合理，但未能遵循用户的明确指令或输入信息。这与内在幻觉高度相关。例如，在摘要任务中，原文明确指出FDA批准了某疫苗，而摘要却称FDA拒绝了它，这既是内在幻 giác，也是忠实性幻觉。

这些重叠但视角不同的分类法（见 **表2** 总结）表明，该领域仍在积极地完善对幻觉的定义。理解这些细微差别至关重要，因为不同类型的幻觉通常源于不同的底层机制，需要特定的检测和缓解策略。

![image.png](https://cdn.jsdelivr.net/gh/gongzitaiyi/picture@master/uPic/2025/11/AyRT73.png)

### 三、幻觉的具体类型与表现

除了核心分类，LLM幻觉还表现为多种具体形式，每种形式都有其独特的特征和影响。

- **事实错误与捏造 (Factual errors and fabrications)**: 包括不正确的事实（如Google Bard曾错误宣称詹姆斯·韦伯太空望远镜拍摄了第一张系外行星图像）和完全捏造的实体/信息（如在法律文书中引用不存在的判例）。
- **上下文不一致 (Contextual inconsistencies)**: 模型输出的信息未出现在提供的上下文中或与之相矛盾。
- **指令不一致/偏离 (Instruction inconsistencies/deviation)**: 模型未能遵循用户的明确指令，例如要求用西班牙语翻译却用英语回答。
- **逻辑不一致 (Logical inconsistencies)**: 输出包含内部逻辑错误或自相矛盾的陈述。
- **时间错乱 (Temporal disorientation)**: 产生过时的、时代错误的或时间上不正确的事实。
- **伦理违规 (Ethical violations)**: 产生有害、诽谤或法律上不正确的内容，例如错误地指控个人犯罪。
- **其他类型**: 还包括**混合幻觉**（Amalgamated hallucinations，错误地组合多个事实）、**无意义回复**（Nonsensical responses）以及**特定于任务的幻觉**（如对话、摘要、代码生成、多模态中的幻觉）。

这些多样化的表现形式凸显了幻觉问题的复杂性，并警示我们“一刀切”的解决方案是行不通的。

### 四、幻觉的深层根源

LLM幻觉源于训练数据、模型架构和用户提示之间复杂的相互作用（见 **表3** 总结）。

![image.png](https://cdn.jsdelivr.net/gh/gongzitaiyi/picture@master/uPic/2025/11/H9UK8J.png)

- **数据相关因素**:
    - **训练数据质量与数量**: 包含错误、噪声或不一致性的数据是产生事实错误回答的直接原因。
    - **数据偏差与代表性不足**: 训练数据中的偏见会导致模型复现“模仿性谬误”。
    - **过时的数据与知识边界**: 静态的训练数据使模型无法处理动态更新的信息，且模型常常无法识别自身知识的边界。
- **模型相关因素**:
    - **自回归（Auto-regressive）性质**: 模型的核心设计是预测下一个最可能的词元，而非确保事实准确性。
    - **架构与训练过程**: 如单向表示的局限性、训练与推理间的暴露偏差（exposure bias）、过度优化特定指标等都可能增加幻觉。
    - **解码策略**: 随机性采样（如高“temperature”设置）会增加创造性，但同时也显著提升了幻觉风险。
    - **过度自信与校准不足**: 模型常以极高的置信度输出错误信息，误导用户。
    - **缺乏推理能力**: 模型主要依赖统计相关性而非真正的因果或逻辑推理。
- **提示相关因素**:
    - **对抗性攻击**: 在提示中故意嵌入虚假信息可能诱导模型产生或放大错误内容，形成“垃圾进，垃圾出”的问题。
    - **提示方法**: 模糊或限制性不足的提示会增加幻觉的概率。

综合来看，幻觉并非简单的bug，而是当前LLM设计范式的一种**涌现属性（emergent property）**。

### 五、幻觉的缓解策略

鉴于幻觉的理论必然性，研究重点已从“完全消除”转向“有效缓解”。策略可分为架构层面和系统层面。

- **架构缓解策略**:
    - **工具增强（Tool-augmented）**: 如 **Toolformer**，让LLM学会在推理时调用外部API、计算器或代码解释器，将事实密集型任务外包给更可靠的专业工具。
    - **通过检索机制进行事实溯源**: **检索增强生成（RAG）** 是最广泛采用的框架之一。它通过在生成前从可信的知识库中检索相关文档，为模型提供事实依据，从而约束其生成内容。
    - **使用合成或对抗性过滤数据进行微调**: 在经过验证的、事实正确的数据集上进行微调，或使用幻觉检测模型过滤掉不佳的输出，以优化模型。
- **系统缓解策略**:
    - **护栏（Guardrails）与符号集成**: 在部署层面应用基于规则的控制机制。**逻辑验证器**可检查输出的内部逻辑，而**事实过滤器**则可将生成的主张与外部知识图谱进行比对。
    - **基于规则的回退机制**: 当模型不确定或输出被标记为潜在幻觉时，系统可以执行预定义的回退策略，如拒绝回答、请求人类介入或要求用户澄清。

未来的方向在于开发结合多种互补策略的 **混合与上下文感知（hybrid and context-aware）** 的缓解系统。

### 结论

LLM幻觉是一个复杂、普遍且理论上不可避免的挑战。它源于数据、模型和交互等多个层面的复杂因素。这一深刻的认知要求我们将战略重心从“根除幻觉”转向开发稳健的检测机制、实施有效的缓解策略，并确保持续的人工监督。
