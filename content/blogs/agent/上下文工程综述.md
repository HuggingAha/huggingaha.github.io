---
title: "Context Engineering综述解读"
showAuthor: false
date: 2025-07-19
description: "context engineering综述"
slug: "context-engineering-survey"
tags: ["论文", "LLM", "ContextEngineering"]
# series: [""]
# series_order: 1
draft: false
---

<!-- 渲染公式 -->
{{< katex >}}


> **代码**：[Meirtz/Awesome-Context-Engineering](https://github.com/Meirtz/Awesome-Context-Engineering.git)  
> **论文**：[A Survey of Context Engineering for Large Language Models](https://arxiv.org/abs/2507.13334)


随着LLM的能力日益强大，我们早已不满足于简单的问答。我们希望构建更复杂、更智能、更可靠的AI系统。这就要求我们必须超越传统的“提示工程（Prompt Engineering）”，进入一个更系统化、更科学的全新范式。最近，一篇名为《A Survey of Context Engineering for Large Language Models》的综述论文为我们系统地梳理了这一领域，今天，我将基于这篇论文，为大家详细解读上下文工程的核心思想与技术蓝图。


---

## 从“炼丹”到“科学”：为什么我们需要上下文工程？

我们都熟悉“提示工程”，它在过去几年里极大地推动了LLM的应用，但它更像一门“艺术”或“炼丹术”，依赖于经验和反复试错。然而，现代AI系统（如AI Agent）的上下文（Context）早已不是一个简单的静态文本字符串，而是一个动态的、结构化的、多方面的信息流。为了系统地设计、管理和优化这些复杂的信息载荷，我们需要一门更严谨的学科——上下文工程。

**上下文工程的正式定义:**

论文为我们提供了一个严谨的数学框架来理解上下文工程。首先，我们知道自回归LLM的原理是最大化给定上下文 \(C\) 后，生成输出序列 \(Y = (y_1, ..., y_T)\) 的条件概率：

$$
P_{\theta}(Y|C) = \prod_{t=1}^{T} P_{\theta}(y_t|y_{<t}, C)
$$

在传统的提示工程中，\(C\) 仅仅是一个静态的提示词（`C = prompt`）。

而上下文工程则将 \(C\) 重新定义为一个由多个信息组件 \((c_1, c_2, ..., c_n)\) 动态编排、组装而成的结构化集合。这个组装过程由一个高级函数 \(A\) 完成：

$$
C = A(c_1, c_2, ..., c_n)
$$

这些信息组件 \(c_i\) 并非随意组合，它们对应了现代AI系统所需信息的各个维度：

- \(c_{instr}\): 系统指令和规则。
- \(c_{know}\): 通过RAG等方式检索到的外部知识。
- \(c_{tools}\): 可用外部工具（API）的定义和签名。
- \(c_{mem}\): 来自先前交互的持久化信息（记忆）。
- \(c_{state}\): 用户、世界或多智能体系统的动态状态。
- \(c_{query}\): 用户的当前直接请求。

基于此，**上下文工程的核心目标**就变成了一个形式化的**优化问题**：寻找一套最优的上下文生成函数 \(F = \{A, \text{Retrieve}, \text{Select}, ...\}\)，使得在任务分布 \(T\) 下，LLM输出质量的期望值最大化。

$$
F^* = \arg\max_F \mathbb{E}_{\tau \sim T} [\text{Reward}(P_\theta(Y|C_F(\tau)), Y^*_\tau)]
$$

其中，\(C_F(\tau)\) 是由函数集 \(F\) 为特定任务 \(\tau\) 生成的上下文，\(Y^*_\tau\) *是理想的输出。这个优化过程还必须遵守模型的最大上下文长度限制 \(|C| \le L_{max}\)*。

为了更清晰地展示两者的区别，论文总结了一张对比表（见表1）：

**表1：提示工程与上下文工程的范式对比**

| 维度 | 提示工程 | 上下文工程 |
| --- | --- | --- |
| **模型** | \(C = \text{prompt}\) (静态字符串) | \(C = A(c_1, c_2, ..., c_n)\) (动态、结构化组装) |
| **目标** | \(\arg\max_{\text{prompt}} P_\theta(Y\text{prompt})\) | \(\arg\max_F \mathbb{E}{\tau \sim T} [\text{Reward}(P\theta(Y|C_F(\tau)), Y^*_\tau)]\) |
| **复杂度** | 手动或自动搜索字符串空间 | 对函数集 \(F\) 进行系统级优化 |
| **信息** | 信息内容在提示中是固定的 | 在约束下最大化任务相关信息 |
| **状态** | 主要是无状态的 | 内在地、显式地管理状态 （\(c_{mem}\), \(c_{state}\)） |
| **可扩展性** | 随着长度和复杂性增加，变得脆弱 | 通过模块化组合管理复杂性 |
| **错误分析** | 手动检查和迭代修正 | 对单个上下文函数进行系统化评估和调试 |

总而言之，上下文工程将我们从“提示词的设计艺术”解放出来，引导我们进入“信息物流与系统优化的科学”领域。

## 上下文工程的技术蓝图：一个系统的分类法

这篇综述论文最大的贡献之一，是提供了一个清晰、全面的技术分类框架（见图1），将上下文工程分解为**三大基础组件 (Foundational Components)** 和 **四大系统实现 (System Implementations)**。

<figure>
  <img src="https://cdn.jsdelivr.net/gh/gongzitaiyi/picture@master/uPic/2025/11/SrAyyx.png" alt="上下文工程分类法总览">
  <figcaption style="text-align: center;">Figure1 上下文工程分类法总览</figcaption>
</figure>


### 三大基础组件：构建上下文的基石

基础组件是上下文工程的“原子操作”，它们构成了处理信息的完整流水线。

**1. 上下文检索与生成 (Context Retrieval and Generation)**
这是上下文的源头。它负责从各种来源获取原始信息。

- **提示工程与上下文生成**：这部分是传统提示工程的超集，包括了如思维链（Chain-of-Thought, CoT）、思想树（Tree-of-Thoughts, ToT）等技术，用于生成结构化的推理路径。
- **外部知识检索**：主要指以检索增强生成（RAG）为代表的技术，从外部知识库（如文档、数据库、知识图谱）中获取信息，以缓解模型的知识过时和幻觉问题。
- **动态上下文组装**：这是一个编排步骤，负责将来自不同来源的信息（用户输入、工具、记忆等）整合成一个连贯、优化的上下文，送给LLM处理。

**2. 上下文处理 (Context Processing)**
获取原始信息后，需要对其进行转换和优化，以最大化其效用。

- **长上下文处理**：为了突破LLM的上下文窗口限制，研究者们开发了多种技术，如高效注意力机制（FlashAttention）、架构创新（Mamba）和位置编码插值等。
- **上下文自我优化与适应**：让LLM具备自我反思和修正的能力。例如，Self-Refine机制允许模型通过迭代反馈来逐步完善输出。
- **多模态及结构化上下文**：处理超越纯文本的信息，如图像、音频、视频，以及知识图谱、表格等结构化数据。

**3. 上下文管理 (Context Management)**
这是关于如何高效地组织、存储和利用上下文信息。

- **基本约束**：处理有限上下文窗口带来的“中间遗忘”（Lost-in-the-Middle）等现象。
- **记忆层次与存储架构**：借鉴操作系统的思想，设计类似虚拟内存的机制（如MemGPT），在快速的上下文窗口（主存）和慢速的外部存储之间进行信息交换，从而实现长期记忆。
- **上下文压缩**：通过技术手段在不损失关键信息的前提下，减少上下文占用的Token数量，从而提升效率。

### 四大系统实现：从理论到实践

当我们将上述基础组件有机地结合起来，便构成了面向应用的复杂AI系统。

**1. 检索增强生成系统 (Retrieval-Augmented Generation, RAG)**
这是目前最主流的应用之一。现代RAG系统已经超越了简单的“检索-生成”模式，发展出了**模块化RAG**（可灵活插拔替换不同组件）、**Agentic RAG**（由AI Agent自主决定何时、如何检索）和**图增强RAG**（利用知识图谱进行更精准、多跳的推理）。

**2. 记忆系统 (Memory Systems)**
为了让AI Agent能够进行长期、有状态的交互，记忆系统至关重要。它通过实现短期工作记忆和长期持久化记忆的协同，让模型能够从过去的经验中学习和适应，表现得更像一个持续存在的智能体，而非一次性的问答机器。

**3. 工具集成推理 (Tool-Integrated Reasoning)**
这使得LLM能够超越其内置知识的限制，通过调用外部API（如搜索引擎、计算器、代码解释器）来与真实世界互动、获取最新信息并执行复杂任务。这从根本上将LLM从一个“文本生成器”转变为一个“世界交互器”。

**4. 多智能体系统 (Multi-Agent Systems)**
这是上下文工程复杂性的顶峰。它涉及多个独立的AI Agent通过复杂的通信协议、编排机制和协调策略进行协作，共同完成单个Agent无法解决的宏大任务。这要求在整个系统中对每个Agent的上下文进行精妙的管理和同步。

## 核心挑战与未来展望

尽管上下文工程取得了巨大进展，但这篇论文也敏锐地指出了该领域面临的一个**根本性不对称（fundamental asymmetry）**：

> 当前的LLM在先进的上下文工程技术加持下，表现出卓越的理解复杂上下文的能力，但在生成同等复杂的长篇输出方面却存在明显的局限性。
> 

简单来说，**模型可以“读懂”一本复杂的书，但很难“写出”一本同样复杂的书**。解决这种“理解-生成”能力之间的鸿沟，是未来研究的重中之重。

展望未来，上下文工程的发展将聚焦于以下几个方向：

- **构建统一的理论基础**：为上下文的效率、组合和优化建立数学模型。
- **探索下一代模型架构**：如状态空间模型（Mamba）等，以更高效地处理超长序列。
- **发展智能上下文组装**：让AI能够自动学习如何为特定任务构建最优上下文。
- **完善评估体系**：开发能够衡量系统动态、多组件行为的“活”基准测试（living benchmarks）。

## 结语

上下文工程标志着AI领域从经验主义的“手工艺时代”迈向了系统科学的“工业化时代”。它为我们提供了一幅清晰的路线图，指导我们如何构建更强大、更可靠的下一代AI系统。

正如这篇综述所总结的，AI系统的性能从根本上取决于其接收到的上下文信息。理解、掌握并系统化地应用上下文工程，将是每一位AI研究者和工程师在未来取得成功的关键。
